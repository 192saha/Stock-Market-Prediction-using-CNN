{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3nrtIpENb0z"
   },
   "source": [
    "# CNN Based Stock Market Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following project, 2D CNN has been used to predict the future stock market fluctuations. \n",
    "\n",
    "Reference Paper - https://www.sciencedirect.com/science/article/pii/S0957417419301915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into training and testing set. Further, the training set is divided into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATADIR = \"./Dataset\"\n",
    "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
    "TRAIN_VALID_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for filename in os.listdir():\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        continue # read only the CSV files\n",
    "    #filepath = os.path.join(DATADIR, filename)\n",
    "    X = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "    name = X[\"Name\"][0]\n",
    "    del X[\"Name\"]\n",
    "    cols = X.columns\n",
    "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "    X.dropna(inplace=True)\n",
    "    # Fit the standard scaler using the training dataset\n",
    "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
    "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
    "    # Save scale transformed dataframe\n",
    "    X[cols] = scaler.transform(X[cols])\n",
    "    data[name] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DJI', 'NASDAQ', 'NSE', 'NYA', 'RUT', 'S&P'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data generator is formed which can produce batches of data when called by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
    "    \"As a generator to produce samples for Keras model\"\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one dataframe from the pool\n",
    "        key = random.choice(list(data.keys()))\n",
    "        df = data[key]\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "        if kind == 'train':\n",
    "            index = index[:split]   # range for the training set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   # range for the validation set\n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)      # pick one time step\n",
    "            n = (df.index == t).argmax()  # find its position in the dataframe\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # can't get enough data for one sequence length\n",
    "            frame = df.iloc[n-seq_len+1:n+1]\n",
    "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
    "            break\n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            X, y = zip(*batch)\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "            yield X, y\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model from the paper is implemented. It contains 2 convolution layers with number of filters = 8. The kernel size are (1x82) and (3x1). This is followed by a max pooling layer which reduces the dimension by half. It is followed by another convolution layer of kernel size = (3 x 1) and a Max Pooling layer. Then it is flattened and fed to a Dense layer which returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics to evaluate our model are F1 score and accuracy. The mean absolute error is used as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = f1_m(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 60, 1, 8)          664       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 29, 1, 8)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 13, 1, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 104)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 104)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1169 (4.57 KB)\n",
      "Trainable params: 1169 (4.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = 60\n",
    "batch_size = 128\n",
    "n_epochs = 20\n",
    "n_features = 82\n",
    " \n",
    "# Produce CNNpred as a binary classification problem\n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "model.summary()  # print model structure to console\n",
    " \n",
    "# Set up callbacks and fit the model\n",
    "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
    "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode=\"max\",\n",
    "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 78s 192ms/step - loss: 0.4352 - acc: 0.5793 - f1macro: 0.4477 - val_loss: 0.4896 - val_acc: 0.5023 - val_f1macro: 0.4529\n",
      "Epoch 2/20\n",
      "  1/400 [..............................] - ETA: 8s - loss: 0.3774 - acc: 0.6797 - f1macro: 0.6763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 75s 188ms/step - loss: 0.3131 - acc: 0.7109 - f1macro: 0.6973 - val_loss: 0.4930 - val_acc: 0.5063 - val_f1macro: 0.4953\n",
      "Epoch 3/20\n",
      "  1/400 [..............................] - ETA: 5s - loss: 0.2573 - acc: 0.7734 - f1macro: 0.7645"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 75s 188ms/step - loss: 0.2636 - acc: 0.7556 - f1macro: 0.7470 - val_loss: 0.5085 - val_acc: 0.4781 - val_f1macro: 0.4501\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 75s 188ms/step - loss: 0.2402 - acc: 0.7759 - f1macro: 0.7685 - val_loss: 0.4625 - val_acc: 0.5352 - val_f1macro: 0.5126\n",
      "Epoch 5/20\n",
      "  6/400 [..............................] - ETA: 4s - loss: 0.2261 - acc: 0.7852 - f1macro: 0.7766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 76s 191ms/step - loss: 0.2224 - acc: 0.7907 - f1macro: 0.7839 - val_loss: 0.4731 - val_acc: 0.5266 - val_f1macro: 0.5137\n",
      "Epoch 6/20\n",
      "  6/400 [..............................] - ETA: 4s - loss: 0.2358 - acc: 0.7760 - f1macro: 0.7677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 72s 179ms/step - loss: 0.2120 - acc: 0.7993 - f1macro: 0.7929 - val_loss: 0.4920 - val_acc: 0.4992 - val_f1macro: 0.4809\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 70s 176ms/step - loss: 0.2030 - acc: 0.8065 - f1macro: 0.7998 - val_loss: 0.4715 - val_acc: 0.5234 - val_f1macro: 0.5161\n",
      "Epoch 8/20\n",
      "  7/400 [..............................] - ETA: 4s - loss: 0.2131 - acc: 0.7980 - f1macro: 0.7933"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 75s 189ms/step - loss: 0.2017 - acc: 0.8076 - f1macro: 0.8019 - val_loss: 0.4615 - val_acc: 0.5406 - val_f1macro: 0.5335\n",
      "Epoch 9/20\n",
      " 11/400 [..............................] - ETA: 4s - loss: 0.1950 - acc: 0.8132 - f1macro: 0.8075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 80s 200ms/step - loss: 0.1940 - acc: 0.8142 - f1macro: 0.8083 - val_loss: 0.4597 - val_acc: 0.5508 - val_f1macro: 0.5477\n",
      "Epoch 10/20\n",
      " 12/400 [..............................] - ETA: 3s - loss: 0.1877 - acc: 0.8190 - f1macro: 0.8161"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 76s 190ms/step - loss: 0.1927 - acc: 0.8144 - f1macro: 0.8084 - val_loss: 0.4559 - val_acc: 0.5516 - val_f1macro: 0.5442\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 77s 193ms/step - loss: 0.1841 - acc: 0.8230 - f1macro: 0.8175 - val_loss: 0.4717 - val_acc: 0.5312 - val_f1macro: 0.5269\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 75s 189ms/step - loss: 0.1858 - acc: 0.8213 - f1macro: 0.8157 - val_loss: 0.4814 - val_acc: 0.5133 - val_f1macro: 0.5093\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 82s 206ms/step - loss: 0.1804 - acc: 0.8265 - f1macro: 0.8213 - val_loss: 0.4832 - val_acc: 0.5172 - val_f1macro: 0.5159\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 75s 187ms/step - loss: 0.1814 - acc: 0.8250 - f1macro: 0.8197 - val_loss: 0.4569 - val_acc: 0.5523 - val_f1macro: 0.5474\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 75s 188ms/step - loss: 0.1784 - acc: 0.8276 - f1macro: 0.8224 - val_loss: 0.4809 - val_acc: 0.5258 - val_f1macro: 0.5208\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 80s 200ms/step - loss: 0.1746 - acc: 0.8306 - f1macro: 0.8253 - val_loss: 0.4977 - val_acc: 0.5016 - val_f1macro: 0.4975\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 80s 201ms/step - loss: 0.1728 - acc: 0.8323 - f1macro: 0.8268 - val_loss: 0.4903 - val_acc: 0.5148 - val_f1macro: 0.5122\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 72s 180ms/step - loss: 0.1708 - acc: 0.8340 - f1macro: 0.8288 - val_loss: 0.4937 - val_acc: 0.5094 - val_f1macro: 0.5038\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 74s 185ms/step - loss: 0.1679 - acc: 0.8361 - f1macro: 0.8313 - val_loss: 0.4590 - val_acc: 0.5430 - val_f1macro: 0.5406\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 72s 181ms/step - loss: 0.1706 - acc: 0.8337 - f1macro: 0.8292 - val_loss: 0.4901 - val_acc: 0.5070 - val_f1macro: 0.5027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c08a46a790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data generator for the test set is also prepared. The mean absolute error, accuracy and F1 score are measured and reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 1ms/step\n",
      "accuracy: 0.5236985236985237\n",
      "MAE: 0.4763014763014763\n",
      "F1: 0.41000962463907603\n"
     ]
    }
   ],
   "source": [
    "def testgen(data, seq_len, targetcol):\n",
    "    \"Return array of all test samples\"\n",
    "    batch = []\n",
    "    for key, df in data.items():\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        # find the start of test sample\n",
    "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
    "        n = (df.index == t).argmax()\n",
    "        for i in range(n+1, len(df)+1):\n",
    "            frame = df.iloc[i-seq_len:i]\n",
    "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
    "    X, y = zip(*batch)\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)\n",
    "\n",
    "# Prepare test data\n",
    "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
    "\n",
    "# Test the model\n",
    "test_out = model.predict(test_data)\n",
    "test_pred = (test_out > 0.5).astype(int)\n",
    "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mpu18ejNf0d"
   },
   "source": [
    "#### In the following section, a modified version of AlexNet has been implemented to improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yNRUxV4k---"
   },
   "source": [
    "## 2D CNN (Improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPHPxKlnY4F2"
   },
   "source": [
    "### 1. Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC4WJSaqQpvP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pqqy6bkQqhH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWDr-ChWZA74"
   },
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39xOXkSVcBB8"
   },
   "source": [
    "The data is split into training and testing set. Further, the training set is divided into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVov7ydcRuJR"
   },
   "outputs": [],
   "source": [
    "#DATADIR = \"./Dataset\"\n",
    "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
    "TRAIN_VALID_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dveDqOlMY3DQ"
   },
   "source": [
    "The data is organised into dictionaries with the keys of the dictionary set to the name of the stock market index. Further the data is scaled using 'Standard Scaler'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXvbymH0Y1ux"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for filename in os.listdir():\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        continue # read only the CSV files\n",
    "    #filepath = os.path.join(DATADIR, filename)\n",
    "    X = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "    name = X[\"Name\"][0]\n",
    "    del X[\"Name\"]\n",
    "    cols = X.columns\n",
    "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "    X.dropna(inplace=True)\n",
    "    # Fit the standard scaler using the training dataset\n",
    "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
    "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
    "    # Save scale transformed dataframe\n",
    "    X[cols] = scaler.transform(X[cols])\n",
    "    data[name] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hmo-sWhsc1A2",
    "outputId": "f5e08437-9b0e-492d-d64f-90d53d2685d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DJI', 'NASDAQ', 'S&P', 'NYA', 'RUT'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEZ0-absliUc"
   },
   "source": [
    "A data generator is formed which can produce batches of data when called by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKhyYJIMli35"
   },
   "outputs": [],
   "source": [
    "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
    "    \"As a generator to produce samples for Keras model\"\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one dataframe from the pool\n",
    "        key = random.choice(list(data.keys()))\n",
    "        df = data[key]\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "        if kind == 'train':\n",
    "            index = index[:split]   # range for the training set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   # range for the validation set\n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)      # pick one time step\n",
    "            n = (df.index == t).argmax()  # find its position in the dataframe\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # can't get enough data for one sequence length\n",
    "            frame = df.iloc[n-seq_len+1:n+1]\n",
    "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
    "            break\n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            X, y = zip(*batch)\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "            yield X, y\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9pWg171koFZ"
   },
   "source": [
    "### 3. Model Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFMBE2zAdZ3v"
   },
   "source": [
    "For my implementation of the AlexNet, I used two convolutional layers of sizes (1x82) and (3x1) with 8 filters. This was followed by a max pooling layer of size (2x1). Then, I followed it with another set of convolutional and max pooling layer, followed by 3 convolutional layers each of size (3x1) and a max pooling layer of size (2x1). This was followed by 3 dense layers of sizes 4096, 4096 and 1. ReLU activation functions was used in all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xONMHr7kaxXy"
   },
   "outputs": [],
   "source": [
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8,8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[3], kernel_size=(3,1), activation=\"relu\"),\n",
    "        Conv2D(n_filters[4], kernel_size=(3,1), activation=\"relu\"),\n",
    "        Conv2D(n_filters[5], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(4096, activation=\"relu\"),\n",
    "        Dense(4096, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpiTfC5GfR00"
   },
   "source": [
    "The metrics to evaluate our model are F1 score and accuracy. The mean absolute error is used as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWbke25CGwXd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = f1_m(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYyO2AhckuoR"
   },
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16p2PTH4ISQk",
    "outputId": "76643e7c-0c6e-4a4b-eb4c-05682284ac6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 60, 1, 8)          664       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 29, 1, 8)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 1, 8)          200       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 9, 1, 8)           200       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 1, 8)           200       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 1, 8)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 24)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,889,473\n",
      "Trainable params: 16,889,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 241s 599ms/step - loss: 0.4434 - acc: 0.5571 - f1macro: 0.3572 - val_loss: 0.4727 - val_acc: 0.5273 - val_f1macro: 0.3447\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 232s 580ms/step - loss: 0.4450 - acc: 0.5550 - f1macro: 0.3564 - val_loss: 0.4805 - val_acc: 0.5195 - val_f1macro: 0.3415\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 225s 562ms/step - loss: 0.4393 - acc: 0.5607 - f1macro: 0.3587 - val_loss: 0.5023 - val_acc: 0.4977 - val_f1macro: 0.3315\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 230s 576ms/step - loss: 0.4423 - acc: 0.5577 - f1macro: 0.3575 - val_loss: 0.4812 - val_acc: 0.5188 - val_f1macro: 0.3406\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 229s 573ms/step - loss: 0.4390 - acc: 0.5610 - f1macro: 0.3589 - val_loss: 0.4883 - val_acc: 0.5117 - val_f1macro: 0.3379\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 232s 580ms/step - loss: 0.4379 - acc: 0.5621 - f1macro: 0.3592 - val_loss: 0.4914 - val_acc: 0.5086 - val_f1macro: 0.3368\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 239s 597ms/step - loss: 0.4423 - acc: 0.5577 - f1macro: 0.3575 - val_loss: 0.5047 - val_acc: 0.4953 - val_f1macro: 0.3309\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 216s 540ms/step - loss: 0.4408 - acc: 0.5592 - f1macro: 0.3581 - val_loss: 0.4742 - val_acc: 0.5258 - val_f1macro: 0.3444\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 217s 541ms/step - loss: 0.4424 - acc: 0.5576 - f1macro: 0.3575 - val_loss: 0.4930 - val_acc: 0.5070 - val_f1macro: 0.3363\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 215s 537ms/step - loss: 0.4403 - acc: 0.5597 - f1macro: 0.3584 - val_loss: 0.4938 - val_acc: 0.5063 - val_f1macro: 0.3359\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 216s 540ms/step - loss: 0.4400 - acc: 0.5600 - f1macro: 0.3584 - val_loss: 0.4719 - val_acc: 0.5281 - val_f1macro: 0.3449\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 216s 540ms/step - loss: 0.4407 - acc: 0.5593 - f1macro: 0.3581 - val_loss: 0.4727 - val_acc: 0.5273 - val_f1macro: 0.3448\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 215s 537ms/step - loss: 0.4453 - acc: 0.5547 - f1macro: 0.3563 - val_loss: 0.5109 - val_acc: 0.4891 - val_f1macro: 0.3281\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 216s 540ms/step - loss: 0.4427 - acc: 0.5573 - f1macro: 0.3574 - val_loss: 0.4797 - val_acc: 0.5203 - val_f1macro: 0.3418\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 215s 537ms/step - loss: 0.4377 - acc: 0.5623 - f1macro: 0.3594 - val_loss: 0.4750 - val_acc: 0.5250 - val_f1macro: 0.3441\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 216s 539ms/step - loss: 0.4375 - acc: 0.5625 - f1macro: 0.3595 - val_loss: 0.4922 - val_acc: 0.5078 - val_f1macro: 0.3361\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 215s 537ms/step - loss: 0.4429 - acc: 0.5571 - f1macro: 0.3573 - val_loss: 0.4828 - val_acc: 0.5172 - val_f1macro: 0.3404\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 214s 536ms/step - loss: 0.4354 - acc: 0.5646 - f1macro: 0.3604 - val_loss: 0.4688 - val_acc: 0.5312 - val_f1macro: 0.3465\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 214s 535ms/step - loss: 0.4403 - acc: 0.5597 - f1macro: 0.3583 - val_loss: 0.4734 - val_acc: 0.5266 - val_f1macro: 0.3444\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 215s 537ms/step - loss: 0.4400 - acc: 0.5600 - f1macro: 0.3584 - val_loss: 0.5039 - val_acc: 0.4961 - val_f1macro: 0.3314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe96d48cbd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 60\n",
    "batch_size = 128\n",
    "n_epochs = 20\n",
    "n_features = 82\n",
    " \n",
    "# Produce CNNpred as a binary classification problem\n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "model.summary()  # print model structure to console\n",
    " \n",
    "# Set up callbacks and fit the model\n",
    "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
    "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode=\"max\",\n",
    "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "]\n",
    "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYGBtI_igDLI"
   },
   "source": [
    "A data generator for the test set is also prepared. The mean absolute error, accuracy and F1 score are measured and reported below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjtf9Tcukzo5"
   },
   "source": [
    "### 5. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhNM5L20KMKe",
    "outputId": "aedbc0d6-647e-453b-e523-1a84958afc6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5375609756097561\n",
      "MAE: 0.4624390243902439\n",
      "F1: 0.6992385786802031\n"
     ]
    }
   ],
   "source": [
    "def testgen(data, seq_len, targetcol):\n",
    "    \"Return array of all test samples\"\n",
    "    batch = []\n",
    "    for key, df in data.items():\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        # find the start of test sample\n",
    "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
    "        n = (df.index == t).argmax()\n",
    "        for i in range(n+1, len(df)+1):\n",
    "            frame = df.iloc[i-seq_len:i]\n",
    "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
    "    X, y = zip(*batch)\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)\n",
    "\n",
    "# Prepare test data\n",
    "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
    "\n",
    "# Test the model\n",
    "test_out = model.predict(test_data)\n",
    "test_pred = (test_out > 0.5).astype(int)\n",
    "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Improved the F1 score from 0.410 to 0.699."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMtSt5ZxlhjUtZXoD79e5EG",
   "include_colab_link": true,
   "name": "Week 9: Implementing AlexNet.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
